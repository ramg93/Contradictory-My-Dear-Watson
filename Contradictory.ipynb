{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b748a6c6",
   "metadata": {},
   "source": [
    "# Download and Unzip the datasets, if these steps haven't been done yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef51e1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle as k\n",
    "import zipfile as Zip\n",
    "import os \n",
    "\n",
    "directory_name = 'contradictory-my-dear-watson'\n",
    "if not directory_name in os.listdir():\n",
    "    !kaggle competitions download -c contradictory-my-dear-watson\n",
    "    with Zip.ZipFile('.'.join([directory_name, 'zip']), 'r') as zip_ref:\n",
    "        zip_ref.extractall(directory_name)\n",
    "    \n",
    "if '.'.join([directory_name, 'zip']) in os.listdir():\n",
    "    os.remove('.'.join([directory_name, 'zip']))\n",
    "    \n",
    "assert directory_name in os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d327431c",
   "metadata": {},
   "source": [
    "# Review our datasets and load them into pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f398b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6623327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Submission Sample has 5195 rows, 2 columns\n",
      " Test Dataset has 5195 rows, 5 columns\n",
      " Train Dataset has 12120 rows, 6 columns\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(directory_name)\n",
    "sub_sample, test, train = [pd.read_csv('/'.join([directory_name, file])) for file in files]\n",
    "\n",
    "print(f' Submission Sample has {sub_sample.shape[0]} rows, {sub_sample.shape[1]} columns')\n",
    "print(f' Test Dataset has {test.shape[0]} rows, {test.shape[1]} columns')\n",
    "print(f' Train Dataset has {train.shape[0]} rows, {train.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7632b1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission Sample has 5195 unique values for id column\n",
      "Submission Sample has 1 unique values for prediction column\n"
     ]
    }
   ],
   "source": [
    "[print(f'Submission Sample has {len(sub_sample[col].unique())} unique values for {col} column') for col in sub_sample.columns];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aff34c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train has 12120 unique values for id column\n",
      "Train has 8209 unique values for premise column\n",
      "Train has 12119 unique values for hypothesis column\n",
      "Train has 15 unique values for lang_abv column\n",
      "Train has 15 unique values for language column\n",
      "Train has 3 unique values for label column\n"
     ]
    }
   ],
   "source": [
    "[print(f'Train has {len(train[col].unique())} unique values for {col} column') for col in train.columns];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a13131d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test has 5195 unique values for id column\n",
      "Test has 4336 unique values for premise column\n",
      "Test has 5195 unique values for hypothesis column\n",
      "Test has 15 unique values for lang_abv column\n",
      "Test has 15 unique values for language column\n"
     ]
    }
   ],
   "source": [
    "[print(f'Test has {len(test[col].unique())} unique values for {col} column') for col in test.columns];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1796f83",
   "metadata": {},
   "source": [
    "# There are many languages inside Train and Test; therefore, in order to help the model stress less about solving for language, all observations will be translated to English. \n",
    "\n",
    "# Going one-by-one takes a long time (circa, 30 minutes per column, per dataset) so we'll throw this step into a multiprocessing pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0813840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting EngTranslator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile EngTranslator.py\n",
    "\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from googletrans import Translator, constants\n",
    "tr = Translator()\n",
    "\n",
    "def toEng(statement = '', lan = ''):\n",
    "    if lan != 'en':\n",
    "        sleep(randint(1, 5))\n",
    "        return tr.translate(statement, dest = 'en').text\n",
    "    else:\n",
    "        return statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f728cc18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 12120/12120 [04:50<00:00, 41.79it/s]\n",
      "100%|███████████████████████████████████| 12120/12120 [04:41<00:00, 42.98it/s]\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "from EngTranslator import toEng\n",
    "\n",
    "data2translate = train.loc[:, ['premise', 'hypothesis', 'lang_abv']].values\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    pool = multiprocessing.Pool(processes = 50)\n",
    "    r_premise = pool.starmap(toEng, tqdm([[p,l] for p, _, l in data2translate]))\n",
    "    r_hypothesis = pool.starmap(toEng, tqdm([[h,l] for _, h, l in data2translate]))\n",
    "    \n",
    "train['eng_premise'] = r_premise\n",
    "train['eng_hypothesis'] = r_hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a1f2c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('/'.join([directory_name, 'trans_train.csv']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26bfc5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data2translate = test.loc[:, ['premise', 'hypothesis', 'lang_abv']].values\n",
    " \n",
    "# if __name__ == '__main__':\n",
    "#     pool = multiprocessing.Pool(processes = 50)\n",
    "#     r_premise = pool.starmap(toEng, tqdm([[p,l] for p, _, l in data2translate]))\n",
    "#     r_hypothesis = pool.starmap(toEng, tqdm([[h,l] for _, h, l in data2translate]))\n",
    "    \n",
    "# test['eng_premise'] = r_premise\n",
    "# test['eng_hypothesis'] = r_hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765eeb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from googletrans import Translator, constants\n",
    "# tr = Translator()\n",
    "# tr.translate('Hello', dest = 'en').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc45b20f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc303746",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
